"""
–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ Saiga-2 –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–≥–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç llama.cpp –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å GGUF –º–æ–¥–µ–ª—è–º–∏.
"""

import os
import logging
from typing import List, Dict, Any, Optional

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å llama_cpp
try:
    from llama_cpp import Llama
    LLAMA_CPP_AVAILABLE = True
except ImportError:
    LLAMA_CPP_AVAILABLE = False
    logger.warning("llama_cpp not available. Install with: pip install llama-cpp-python")


class SaigaNotInstalledError(Exception):
    """–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å Saiga-2 –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"""
    pass


class SaigaFallbackLawyer:
    """
    Fallback –∫–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã –±–µ–∑ llama_cpp.
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
    """
    
    def __init__(self):
        self.is_loaded = True  # –í—Å–µ–≥–¥–∞ –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ
        
    def load_model(self) -> bool:
        """Fallback –º–æ–¥–µ–ª—å –≤—Å–µ–≥–¥–∞ –≥–æ—Ç–æ–≤–∞"""
        return True
        
    def is_model_available(self) -> bool:
        """Fallback –º–æ–¥–µ–ª—å –≤—Å–µ–≥–¥–∞ –¥–æ—Å—Ç—É–ø–Ω–∞"""
        return True
        
    def generate_answer(self, question: str, context_chunks: List[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è LLM.
        
        Args:
            question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            context_chunks: –ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
            
        Returns:
            Dict: –û—Ç–≤–µ—Ç —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        if not context_chunks:
            return {
                'success': True,
                'answer': '–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å.',
                'context_used': [],
                'tokens_used': 0,
                'model': 'Fallback (–±–µ–∑ LLM)'
            }
        
        # –°–æ—Å—Ç–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
        context_text = "\n\n".join([chunk['chunk'] for chunk in context_chunks[:3]])
        sources = list(set([chunk['metadata']['source'] for chunk in context_chunks[:3]]))
        
        answer = f"""–ù–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –ø—Ä–∞–≤–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö:

{context_text}

**–ò—Å—Ç–æ—á–Ω–∏–∫–∏:** {', '.join(sources)}

*–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –≠—Ç–æ—Ç –æ—Ç–≤–µ—Ç —Å–æ—Å—Ç–∞–≤–ª–µ–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ llama-cpp-python –∏ –º–æ–¥–µ–ª—å Saiga-2.*"""
        
        return {
            'success': True,
            'answer': answer,
            'context_used': context_chunks[:3],
            'tokens_used': len(answer.split()),
            'model': 'Fallback (–ø–æ–∏—Å–∫ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º)'
        }
    
    def get_model_info(self) -> Dict[str, Any]:
        """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ fallback –º–æ–¥–µ–ª–∏"""
        return {
            'model_type': 'Fallback',
            'model_loaded': True,
            'model_file_exists': False,
            'model_size_mb': 0,
            'context_length': 0,
            'description': '–†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã –±–µ–∑ LLM –º–æ–¥–µ–ª–∏'
        }


class SaigaLawyer:
    """
    –Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –Ω–∞ –±–∞–∑–µ –º–æ–¥–µ–ª–∏ Saiga-2.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç GGUF –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ llama_cpp –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤.
    """
    
    def __init__(self, model_path: str = "models/saiga/saiga2.gguf"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Saiga-2 –º–æ–¥–µ–ª–∏.
        
        Args:
            model_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –º–æ–¥–µ–ª–∏ GGUF
        """
        self.model_path = model_path
        self.model = None
        self.is_loaded = False
        
    def load_model(self) -> bool:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å Saiga-2.
        
        Returns:
            bool: True –µ—Å–ª–∏ –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞
            
        Raises:
            SaigaNotInstalledError: –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞
        """
        if not LLAMA_CPP_AVAILABLE:
            raise SaigaNotInstalledError(
                "llama_cpp –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install llama-cpp-python"
            )
        
        if not os.path.exists(self.model_path):
            raise SaigaNotInstalledError(
                f"–ú–æ–¥–µ–ª—å Saiga-2 –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {self.model_path}\n"
                "–°–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª—å —Å: https://huggingface.co/IlyaGusev/saiga2_7b_gguf"
            )
        
        try:
            logger.info(f"–ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å Saiga-2 –∏–∑ {self.model_path}...")
            
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã
            self.model = Llama(
                model_path=self.model_path,
                n_ctx=4096,          # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ
                n_threads=4,         # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤
                n_gpu_layers=0,      # GPU —Å–ª–æ–∏ (0 = —Ç–æ–ª—å–∫–æ CPU)
                verbose=False,       # –û—Ç–∫–ª—é—á–∞–µ–º –ø–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥
                use_mmap=True,       # –ò—Å–ø–æ–ª—å–∑—É–µ–º memory mapping
                use_mlock=False,     # –ù–µ –±–ª–æ–∫–∏—Ä—É–µ–º –ø–∞–º—è—Ç—å
            )
            
            self.is_loaded = True
            logger.info("‚úÖ –ú–æ–¥–µ–ª—å Saiga-2 —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {str(e)}")
            raise SaigaNotInstalledError(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {str(e)}")
    
    def _build_prompt(self, question: str, context_chunks: List[Dict[str, Any]]) -> str:
        """
        –°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è –º–æ–¥–µ–ª–∏ –≤ –Ω—É–∂–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ.
        
        Args:
            question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            context_chunks: –ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            
        Returns:
            str: –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        """
        # –°–∏—Å—Ç–µ–º–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è
        system_prompt = """–¢—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π —é—Ä–∏—Å—Ç, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—â–∏–π—Å—è –Ω–∞ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–µ –†–µ—Å–ø—É–±–ª–∏–∫–∏ –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω.

–ü–†–ê–í–ò–õ–ê:
- –û—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
- –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ —Ñ–∞–∫—Ç—ã –∏–∑ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å–Ω—ã—Ö –∞–∫—Ç–æ–≤
- –£–∫–∞–∑—ã–≤–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
- –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º
- –ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –Ω–æ—Ä–º—ã
- –û—Ç–≤–µ—á–∞–π —á–µ—Ç–∫–æ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê:
1. –ö—Ä–∞—Ç–∫–∏–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å
2. –°—Å—ã–ª–∫–∏ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Å—Ç–∞—Ç—å–∏
3. –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –ø—Ä–∏–º–µ–Ω–∏–º—ã—Ö –Ω–æ—Ä–º
4. –ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ - –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏"""

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        context_text = ""
        if context_chunks:
            context_text = "–ö–û–ù–¢–ï–ö–°–¢ –ò–ó –ó–ê–ö–û–ù–û–î–ê–¢–ï–õ–¨–°–¢–í–ê:\n\n"
            for i, chunk in enumerate(context_chunks, 1):
                source = chunk['metadata']['source']
                content = chunk['chunk']
                relevance = chunk['score']
                
                context_text += f"[–ò—Å—Ç–æ—á–Ω–∏–∫ {i}: {source} (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {relevance:.2f})]\n"
                context_text += f"{content}\n\n"
        else:
            context_text = "–ö–û–ù–¢–ï–ö–°–¢: –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤ –±–∞–∑–µ –∑–∞–∫–æ–Ω–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.\n\n"
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        full_prompt = f"""<s>system
{system_prompt}</s>

<s>user
{context_text}

–í–û–ü–†–û–° –ö–õ–ò–ï–ù–¢–ê:
{question}

–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—É—é –ø—Ä–∞–≤–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏ –¥–∞–π –ø–æ–¥—Ä–æ–±–Ω—ã–π, —é—Ä–∏–¥–∏—á–µ—Å–∫–∏ –≥—Ä–∞–º–æ—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç.</s>

<s>assistant
"""
        
        return full_prompt
    
    def answer(self, question: str, context_chunks: List[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å.
        
        Args:
            question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            context_chunks: –ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
            
        Returns:
            Dict: –û—Ç–≤–µ—Ç —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        if not self.is_loaded:
            try:
                self.load_model()
            except SaigaNotInstalledError:
                raise
        
        try:
            # –°—Ç—Ä–æ–∏–º –ø—Ä–æ–º–ø—Ç
            prompt = self._build_prompt(question, context_chunks or [])
            
            logger.info("ü§ñ –ì–µ–Ω–µ—Ä–∏—Ä—É—é –æ—Ç–≤–µ—Ç —Å –ø–æ–º–æ—â—å—é Saiga-2...")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            response = self.model(
                prompt,
                max_tokens=500,        # –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤
                temperature=0.2,       # –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
                top_p=0.9,            # Top-p sampling
                top_k=40,             # Top-k sampling
                repeat_penalty=1.1,   # –®—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è
                stop=["</s>", "<s>", "user:", "system:", "assistant:"],  # –°—Ç–æ–ø-—Ç–æ–∫–µ–Ω—ã
                echo=False,           # –ù–µ –ø–æ–≤—Ç–æ—Ä—è—Ç—å –ø—Ä–æ–º–ø—Ç
            )
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞
            answer_text = response['choices'][0]['text'].strip()
            
            # –û—á–∏—â–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç —Å–ª—É–∂–µ–±–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
            for stop_token in ["</s>", "<s>", "user", "system", "assistant"]:
                answer_text = answer_text.replace(stop_token, "").strip()
            
            logger.info("‚úÖ –û—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω!")
            
            return {
                'answer': answer_text,
                'context_used': context_chunks or [],
                'question': question,
                'model_used': 'Saiga-2 7B',
                'context_count': len(context_chunks) if context_chunks else 0,
                'tokens_used': response['usage']['total_tokens'],
                'success': True
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {str(e)}")
            return {
                'answer': f'–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {str(e)}',
                'context_used': context_chunks or [],
                'question': question,
                'model_used': 'Saiga-2 7B (ERROR)',
                'error': str(e),
                'success': False
            }
    
    def is_model_available(self) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –±–µ–∑ –ø–æ–ø—ã—Ç–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏.
        
        Returns:
            bool: True –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –¥–æ—Å—Ç—É–ø–Ω–∞
        """
        return (LLAMA_CPP_AVAILABLE and 
                os.path.exists(self.model_path) and 
                os.path.getsize(self.model_path) > 1000000)  # –ë–æ–ª—å—à–µ 1MB
    
    def get_model_info(self) -> Dict[str, Any]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏.
        
        Returns:
            Dict: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
        """
        info = {
            'model_path': self.model_path,
            'llama_cpp_available': LLAMA_CPP_AVAILABLE,
            'model_file_exists': os.path.exists(self.model_path),
            'model_loaded': self.is_loaded,
            'model_size_mb': 0
        }
        
        if os.path.exists(self.model_path):
            info['model_size_mb'] = os.path.getsize(self.model_path) / (1024 * 1024)
        
        return info


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –¥—Ä—É–≥–∏—Ö –º–æ–¥—É–ª—è—Ö (–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è –ø–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—é)
saiga_lawyer = None

def initialize_saiga():
    """Initialize Saiga model (call this from Streamlit app)"""
    global saiga_lawyer
    if saiga_lawyer is None:
        if not LLAMA_CPP_AVAILABLE:
            logger.info("llama_cpp not available, using fallback mode")
            saiga_lawyer = SaigaFallbackLawyer()
            return True
        
        saiga_lawyer = SaigaLawyer()
        
        try:
            success = saiga_lawyer.load_model()
            if success:
                logger.info("Saiga model initialized successfully")
                return True
            else:
                logger.warning("Failed to initialize Saiga model, using fallback")
                saiga_lawyer = SaigaFallbackLawyer()
                return True
        except Exception as e:
            logger.error(f"Error initializing Saiga: {e}, using fallback")
            saiga_lawyer = SaigaFallbackLawyer()
            return True
    return True


def generate_answer_with_saiga(question: str, context_chunks: List[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    –£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ —Å Saiga-2.
    
    Args:
        question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        context_chunks: –ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
        
    Returns:
        Dict: –û—Ç–≤–µ—Ç —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        
    Raises:
        SaigaNotInstalledError: –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞
    """
    global saiga_lawyer
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞
    if saiga_lawyer is None:
        if not initialize_saiga():
            return {
                'success': False,
                'error': 'Failed to initialize Saiga model',
                'answer': '–ò–∑–≤–∏–Ω–∏—Ç–µ, –º–æ–¥–µ–ª—å Saiga-2 –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —É—Å—Ç–∞–Ω–æ–≤–∫—É llama-cpp-python.'
            }
    
    return saiga_lawyer.generate_answer(question, context_chunks)


def is_saiga_available() -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Saiga-2 –º–æ–¥–µ–ª–∏ (–≤–∫–ª—é—á–∞—è fallback —Ä–µ–∂–∏–º).
    
    Returns:
        bool: True –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –¥–æ—Å—Ç—É–ø–Ω–∞ (–≤–∫–ª—é—á–∞—è fallback)
    """
    global saiga_lawyer
    
    # Fallback —Ä–µ–∂–∏–º –≤—Å–µ–≥–¥–∞ –¥–æ—Å—Ç—É–ø–µ–Ω
    if not LLAMA_CPP_AVAILABLE:
        return True
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ –º–æ–¥–µ–ª–∏
    model_path = "models/saiga/saiga2.gguf"
    if not os.path.exists(model_path):
        return True  # Fallback —Ä–µ–∂–∏–º –¥–æ—Å—Ç—É–ø–µ–Ω
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if saiga_lawyer is None:
        return initialize_saiga()
    
    return saiga_lawyer.is_model_available()


if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Saiga-2 –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å
    if is_saiga_available():
        print("‚úÖ –ú–æ–¥–µ–ª—å Saiga-2 –¥–æ—Å—Ç—É–ø–Ω–∞")
        
        # –¢–µ—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        test_context = [{
            'chunk': '–°–æ–±—Å—Ç–≤–µ–Ω–Ω–∏–∫—É –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∞—Ç –ø—Ä–∞–≤–∞ –≤–ª–∞–¥–µ–Ω–∏—è, –ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏ —Ä–∞—Å–ø–æ—Ä—è–∂–µ–Ω–∏—è —Å–≤–æ–∏–º –∏–º—É—â–µ—Å—Ç–≤–æ–º.',
            'metadata': {'source': 'civil_code_kz.txt'},
            'score': 0.9,
            'rank': 1
        }]
        
        try:
            # –¢–µ—Å—Ç–æ–≤—ã–π –≤–æ–ø—Ä–æ—Å
            result = generate_answer_with_saiga(
                "–ß—Ç–æ —Ç–∞–∫–æ–µ –ø—Ä–∞–≤–æ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏?", 
                test_context
            )
            
            if result['success']:
                print("‚úÖ –¢–µ—Å—Ç –ø—Ä–æ—à–µ–ª —É—Å–ø–µ—à–Ω–æ!")
                print(f"–û—Ç–≤–µ—Ç: {result['answer'][:100]}...")
                print(f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤: {result.get('tokens_used', 'N/A')}")
            else:
                print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –æ—Ç–≤–µ—Ç–µ: {result.get('error', 'Unknown')}")
                
        except SaigaNotInstalledError as e:
            print(f"‚ùå {e}")
        except Exception as e:
            print(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
    else:
        print("‚ùå –ú–æ–¥–µ–ª—å Saiga-2 –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏
        info = saiga_lawyer.get_model_info()
        print("üìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏:")
        for key, value in info.items():
            print(f"  {key}: {value}")
        
        if not info['llama_cpp_available']:
            print("\nüí° –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ llama-cpp-python: pip install llama-cpp-python")
        
        if not info['model_file_exists']:
            print(f"\nüí° –°–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª—å –≤: {info['model_path']}")
            print("   –°—Å—ã–ª–∫–∞: https://huggingface.co/IlyaGusev/saiga2_7b_gguf")